{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Прогноз часового спроса такси по районам — улучшенная версия"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6aff30c8262e8007"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Пайплайн предобработки / FE / генерации данных"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c009d4aebe5ba04"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import gdown\n",
    "import holidays\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from meteostat import Stations, Hourly\n",
    "from datetime import datetime\n",
    "from pathlib import Path"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-30T18:02:55.078809Z",
     "start_time": "2025-06-30T18:02:54.691648Z"
    }
   },
   "id": "d3ac6d4782c444ca",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_raw_data() -> pd.DataFrame:\n",
    "    current_dir = Path().resolve()\n",
    "    target_dir = current_dir.parent / 'mfdp_data'\n",
    "\n",
    "    file_paths = [f for f in target_dir.iterdir() if f.is_file()]\n",
    "    dfs = []\n",
    "    for path in file_paths:\n",
    "        df = pd.read_parquet(path)\n",
    "        fname = str(path).lower()\n",
    "        if 'yellow' in fname:\n",
    "            df = df.rename(columns={\n",
    "                'tpep_pickup_datetime': 'pickup_datetime',\n",
    "                'PULocationID': 'location_id',\n",
    "                'trip_distance': 'distance',\n",
    "                'fare_amount': 'cost',\n",
    "            })\n",
    "        elif 'green' in fname:\n",
    "            df = df.rename(columns={\n",
    "                'lpep_pickup_datetime': 'pickup_datetime',\n",
    "                'PULocationID': 'location_id',\n",
    "                'trip_distance': 'distance',\n",
    "                'fare_amount': 'cost',\n",
    "            })\n",
    "        elif 'fhvhv' in fname:\n",
    "            df = df.rename(columns={\n",
    "                'pickup_datetime': 'pickup_datetime', \n",
    "                'PULocationID': 'location_id',\n",
    "                'trip_miles': 'distance',\n",
    "                'base_passenger_fare': 'cost',\n",
    "            })\n",
    "        if ('pickup_datetime' in df.columns and 'location_id' in df.columns and\n",
    "                'distance' in df.columns and 'cost' in df.columns):\n",
    "            df = df[['pickup_datetime', 'location_id', 'distance', 'cost']]\n",
    "            allowed_ids = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 212, 213, 214, 215, 217, 218, 219, 220, 221, 222, 223, 224, 227, 235, 240, 241, 242, 243, 245, 248, 250, 251, 252, 253, 254, 257, 258, 259, 260, 264, 265]\n",
    "\n",
    "            df = df[df['location_id'].isin(allowed_ids)]\n",
    "\n",
    "            df = df[(df['distance'] > 0) & (df['cost'] > 0)]\n",
    "            df['cost_per_mile'] = df['cost'] / df['distance']\n",
    "            \n",
    "            df['cost_per_mile'] = df['cost_per_mile'].replace([float('inf'), -float('inf')], None)\n",
    "            df = df[df['cost_per_mile'] < 100]\n",
    "            df = df.drop(columns=['cost', 'distance'])\n",
    "            df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'], errors='coerce')\n",
    "            df = df.dropna().drop_duplicates()\n",
    "            dfs.append(df)\n",
    "    combined = pd.concat(dfs, ignore_index=True).drop_duplicates().reset_index(drop=True)\n",
    "    combined = combined[(combined['pickup_datetime'] >= '2024-01-01') & (combined['pickup_datetime'] < '2025-04-01')]\n",
    "    combined['date'] = combined['pickup_datetime'].dt.date\n",
    "    combined['hour'] = combined['pickup_datetime'].dt.hour\n",
    "    combined = combined.drop(columns=['pickup_datetime'])\n",
    "    return combined"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-30T18:02:55.086829Z",
     "start_time": "2025-06-30T18:02:55.078809Z"
    }
   },
   "id": "71038789cb42be63",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def aggregate_trips(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    grouped = df.groupby(['date', 'hour', 'location_id']).agg(\n",
    "        avg_cost_per_mile=('cost_per_mile', 'mean')\n",
    "    ).reset_index()\n",
    "    grouped['trips_count'] = df.groupby(['date', 'hour', 'location_id']).size().values\n",
    "    return grouped"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-30T18:02:55.090420Z",
     "start_time": "2025-06-30T18:02:55.086829Z"
    }
   },
   "id": "848857260138675d",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    us_holidays = holidays.US()\n",
    "    non_working = {'New Year\\'s Day', 'MLK Day', 'Washington\\'s Birthday', 'Memorial Day',\n",
    "                   'Juneteenth', 'Independence Day', 'Labor Day', 'Thanksgiving', 'Christmas Day'}\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['day_of_week'] = df['date'].dt.dayofweek\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "    df['is_holiday'] = df['date'].apply(lambda d: us_holidays.get(d.date()) in non_working)\n",
    "    df['is_month_start'] = df['date'].dt.is_month_start.astype(int)\n",
    "    df['is_month_end'] = df['date'].dt.is_month_end.astype(int)\n",
    "    df['day_of_year'] = df['date'].dt.dayofyear\n",
    "    df['week_of_year'] = df['date'].dt.isocalendar().week.astype(int)\n",
    "    df['is_pre_holiday'] = df['date'].shift(-1).apply(lambda d: us_holidays.get(d.date()) in non_working)\n",
    "    df['is_post_holiday'] = df['date'].shift(1).apply(lambda d: us_holidays.get(d.date()) in non_working)\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-30T18:02:55.095401Z",
     "start_time": "2025-06-30T18:02:55.091424Z"
    }
   },
   "id": "889e3209881f1e70",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def merge_weather(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    start = datetime(2024, 1, 1)\n",
    "    end = datetime(2025, 5, 1)\n",
    "    station = Stations().nearby(40.7128, -74.0060).fetch(1).index[0]\n",
    "    weather = Hourly(station, start, end).fetch().reset_index()\n",
    "    weather['date'] = weather['time'].dt.date\n",
    "    weather['hour'] = weather['time'].dt.hour\n",
    "    weather = weather[['date', 'hour', 'temp', 'prcp', 'wspd']]\n",
    "    weather['date'] = pd.to_datetime(weather['date'])\n",
    "    return df.merge(weather, on=['date', 'hour'], how='left')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-30T18:02:55.099846Z",
     "start_time": "2025-06-30T18:02:55.096405Z"
    }
   },
   "id": "d279b513fbc5c2",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def save_data(df: pd.DataFrame, output: str='result_lstm_cost.csv') -> pd.DataFrame:\n",
    "    df['datetime'] = pd.to_datetime(df['date'].dt.strftime('%Y-%m-%d') + ' ' + df['hour'].astype(str) + ':00')\n",
    "    df = df.sort_values(['location_id', 'datetime']).reset_index(drop=True)\n",
    "    df = df.drop(['datetime'], axis=1)\n",
    "    df.to_csv(output, index=False)\n",
    "    df.head(n=15).to_csv('result_lstm_test_cost.csv', index=False)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-30T18:02:55.103504Z",
     "start_time": "2025-06-30T18:02:55.100850Z"
    }
   },
   "id": "3f3f8adb6adacbbb",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def pipeline() -> pd.DataFrame:\n",
    "    df = load_raw_data()\n",
    "    df = aggregate_trips(df)\n",
    "    df = engineer_features(df)\n",
    "    df = merge_weather(df)\n",
    "    df = save_data(df)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-30T18:02:55.106634Z",
     "start_time": "2025-06-30T18:02:55.103504Z"
    }
   },
   "id": "dfd216db996ee2cd",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\mfdp\\.venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py:128\u001B[39m, in \u001B[36m_pseudo_sync_runner\u001B[39m\u001B[34m(coro)\u001B[39m\n\u001B[32m    120\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    121\u001B[39m \u001B[33;03mA runner that does not really allow async execution, and just advance the coroutine.\u001B[39;00m\n\u001B[32m    122\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    125\u001B[39m \u001B[33;03mCredit to Nathaniel Smith\u001B[39;00m\n\u001B[32m    126\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    127\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m128\u001B[39m     \u001B[43mcoro\u001B[49m\u001B[43m.\u001B[49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m    129\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m    130\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m exc.value\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\mfdp\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3384\u001B[39m, in \u001B[36mInteractiveShell.run_cell_async\u001B[39m\u001B[34m(self, raw_cell, store_history, silent, shell_futures, transformed_cell, preprocessing_exc_tuple, cell_id)\u001B[39m\n\u001B[32m   3380\u001B[39m exec_count = \u001B[38;5;28mself\u001B[39m.execution_count\n\u001B[32m   3381\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m result.error_in_exec:\n\u001B[32m   3382\u001B[39m     \u001B[38;5;66;03m# Store formatted traceback and error details\u001B[39;00m\n\u001B[32m   3383\u001B[39m     \u001B[38;5;28mself\u001B[39m.history_manager.exceptions[exec_count] = (\n\u001B[32m-> \u001B[39m\u001B[32m3384\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_format_exception_for_storage\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m.\u001B[49m\u001B[43merror_in_exec\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3385\u001B[39m     )\n\u001B[32m   3387\u001B[39m \u001B[38;5;66;03m# Each cell is a *single* input, regardless of how many lines it has\u001B[39;00m\n\u001B[32m   3388\u001B[39m \u001B[38;5;28mself\u001B[39m.execution_count += \u001B[32m1\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\mfdp\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3438\u001B[39m, in \u001B[36mInteractiveShell._format_exception_for_storage\u001B[39m\u001B[34m(self, exception, filename, running_compiled_code)\u001B[39m\n\u001B[32m   3435\u001B[39m         stb = evalue._render_traceback_()\n\u001B[32m   3436\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   3437\u001B[39m         \u001B[38;5;66;03m# Otherwise, use InteractiveTB to format the traceback.\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m3438\u001B[39m         stb = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mInteractiveTB\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstructured_traceback\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   3439\u001B[39m \u001B[43m            \u001B[49m\u001B[43metype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtb_offset\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\n\u001B[32m   3440\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3441\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m   3442\u001B[39m     \u001B[38;5;66;03m# In case formatting fails, fallback to Python's built-in formatting.\u001B[39;00m\n\u001B[32m   3443\u001B[39m     stb = traceback.format_exception(etype, evalue, tb)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\mfdp\\.venv\\Lib\\site-packages\\IPython\\core\\ultratb.py:1182\u001B[39m, in \u001B[36mAutoFormattedTB.structured_traceback\u001B[39m\u001B[34m(self, etype, evalue, etb, tb_offset, context)\u001B[39m\n\u001B[32m   1180\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1181\u001B[39m     \u001B[38;5;28mself\u001B[39m.tb = etb\n\u001B[32m-> \u001B[39m\u001B[32m1182\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mFormattedTB\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstructured_traceback\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1183\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43metype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43metb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtb_offset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontext\u001B[49m\n\u001B[32m   1184\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\mfdp\\.venv\\Lib\\site-packages\\IPython\\core\\ultratb.py:1053\u001B[39m, in \u001B[36mFormattedTB.structured_traceback\u001B[39m\u001B[34m(self, etype, evalue, etb, tb_offset, context)\u001B[39m\n\u001B[32m   1050\u001B[39m mode = \u001B[38;5;28mself\u001B[39m.mode\n\u001B[32m   1051\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m mode \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.verbose_modes:\n\u001B[32m   1052\u001B[39m     \u001B[38;5;66;03m# Verbose modes need a full traceback\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1053\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVerboseTB\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstructured_traceback\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1054\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43metype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43metb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtb_offset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontext\u001B[49m\n\u001B[32m   1055\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1056\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m mode == \u001B[33m\"\u001B[39m\u001B[33mDocs\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m   1057\u001B[39m     \u001B[38;5;66;03m# return DocTB\u001B[39;00m\n\u001B[32m   1058\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m DocTB(\n\u001B[32m   1059\u001B[39m         theme_name=\u001B[38;5;28mself\u001B[39m._theme_name,\n\u001B[32m   1060\u001B[39m         call_pdb=\u001B[38;5;28mself\u001B[39m.call_pdb,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1068\u001B[39m         etype, evalue, etb, tb_offset, \u001B[32m1\u001B[39m\n\u001B[32m   1069\u001B[39m     )  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\mfdp\\.venv\\Lib\\site-packages\\IPython\\core\\ultratb.py:861\u001B[39m, in \u001B[36mVerboseTB.structured_traceback\u001B[39m\u001B[34m(self, etype, evalue, etb, tb_offset, context)\u001B[39m\n\u001B[32m    852\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mstructured_traceback\u001B[39m(\n\u001B[32m    853\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    854\u001B[39m     etype: \u001B[38;5;28mtype\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    858\u001B[39m     context: \u001B[38;5;28mint\u001B[39m = \u001B[32m5\u001B[39m,\n\u001B[32m    859\u001B[39m ) -> \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[32m    860\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m861\u001B[39m     formatted_exceptions: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m]] = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mformat_exception_as_a_whole\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    862\u001B[39m \u001B[43m        \u001B[49m\u001B[43metype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43metb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtb_offset\u001B[49m\n\u001B[32m    863\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    865\u001B[39m     termsize = \u001B[38;5;28mmin\u001B[39m(\u001B[32m75\u001B[39m, get_terminal_size()[\u001B[32m0\u001B[39m])\n\u001B[32m    866\u001B[39m     theme = theme_table[\u001B[38;5;28mself\u001B[39m._theme_name]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\mfdp\\.venv\\Lib\\site-packages\\IPython\\core\\ultratb.py:746\u001B[39m, in \u001B[36mVerboseTB.format_exception_as_a_whole\u001B[39m\u001B[34m(self, etype, evalue, etb, context, tb_offset)\u001B[39m\n\u001B[32m    744\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(tb_offset, \u001B[38;5;28mint\u001B[39m)\n\u001B[32m    745\u001B[39m head = \u001B[38;5;28mself\u001B[39m.prepare_header(\u001B[38;5;28mstr\u001B[39m(etype), \u001B[38;5;28mself\u001B[39m.long_header)\n\u001B[32m--> \u001B[39m\u001B[32m746\u001B[39m records = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_records\u001B[49m\u001B[43m(\u001B[49m\u001B[43metb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtb_offset\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m etb \u001B[38;5;28;01melse\u001B[39;00m []\n\u001B[32m    748\u001B[39m frames = []\n\u001B[32m    749\u001B[39m skipped = \u001B[32m0\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\mfdp\\.venv\\Lib\\site-packages\\IPython\\core\\ultratb.py:819\u001B[39m, in \u001B[36mVerboseTB.get_records\u001B[39m\u001B[34m(self, etb, context, tb_offset)\u001B[39m\n\u001B[32m    817\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m cf \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    818\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m819\u001B[39m         mod = \u001B[43minspect\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgetmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcf\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtb_frame\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    820\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m mod \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    821\u001B[39m             mod_name = mod.\u001B[34m__name__\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\inspect.py:1006\u001B[39m, in \u001B[36mgetmodule\u001B[39m\u001B[34m(object, _filename)\u001B[39m\n\u001B[32m   1004\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m ismodule(module) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(module, \u001B[33m'\u001B[39m\u001B[33m__file__\u001B[39m\u001B[33m'\u001B[39m):\n\u001B[32m   1005\u001B[39m     f = module.\u001B[34m__file__\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1006\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m f == \u001B[43m_filesbymodname\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m:\n\u001B[32m   1007\u001B[39m         \u001B[38;5;66;03m# Have already mapped this module, so skip it\u001B[39;00m\n\u001B[32m   1008\u001B[39m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[32m   1009\u001B[39m     _filesbymodname[modname] = f\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "source": [
    "df1 = load_raw_data()\n",
    "df1.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-30T18:03:25.312569Z",
     "start_time": "2025-06-30T18:02:55.106634Z"
    }
   },
   "id": "b394f57c49f0c417",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df1[df1['location_id'] == 2].head(1000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-30T18:03:25.313575Z"
    }
   },
   "id": "2d66193903697476",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df2 = aggregate_trips(df1)\n",
    "df2.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-30T18:03:25.314575Z",
     "start_time": "2025-06-30T18:03:25.313575Z"
    }
   },
   "id": "18e61e511d4b930f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df3 = engineer_features(df2)\n",
    "df3.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-30T18:03:25.315575Z",
     "start_time": "2025-06-30T18:03:25.314575Z"
    }
   },
   "id": "5d949550f9ac6a03",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df4 = merge_weather(df3)\n",
    "df4.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-30T18:03:25.315575Z"
    }
   },
   "id": "17fb20229ce9a130",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df5 = save_data(df4)\n",
    "df5.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-30T18:03:25.316575Z",
     "start_time": "2025-06-30T18:03:25.316575Z"
    }
   },
   "id": "6f3d311c0314037e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b440f35190bfeca0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "df = pd.read_csv('result_lstm_cost.csv', parse_dates=['date'])\n",
    "\n",
    "features = [\n",
    "    'location_id', 'trips_count', 'hour', 'temp', 'prcp', 'wspd',\n",
    "    'day_of_week', 'month', 'is_weekend', 'is_holiday',\n",
    "    'is_month_start', 'is_month_end', 'day_of_year', 'week_of_year',\n",
    "    'is_pre_holiday', 'is_post_holiday'\n",
    "]\n",
    "target = 'avg_cost_per_mile'\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, y_trainval, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "categorical_features = ['location_id']\n",
    "numeric_features = [c for c in features if c not in categorical_features]\n",
    "\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cat', categorical_transformer, categorical_features),\n",
    "    ('num', numeric_transformer, numeric_features)\n",
    "])\n",
    "\n",
    "X_train_proc = preprocessor.fit_transform(X_train)\n",
    "X_val_proc   = preprocessor.transform(X_val)\n",
    "X_test_proc  = preprocessor.transform(X_test)\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        arr = X.toarray() if hasattr(X, \"toarray\") else X\n",
    "        self.X = torch.tensor(arr, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.values if hasattr(y, \"values\") else y,\n",
    "                              dtype=torch.float32).unsqueeze(1)\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = TabularDataset(X_train_proc, y_train)\n",
    "val_ds   = TabularDataset(X_val_proc,   y_val)\n",
    "test_ds  = TabularDataset(X_test_proc,  y_test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=64)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=64)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-30T19:29:31.873338Z",
     "start_time": "2025-06-30T19:29:22.530984Z"
    }
   },
   "id": "d77ecae99040ac4a",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## Training **XGBoost**"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "**XGBoost** training completed."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Saved **XGBoost** model to `xgboost.joblib`."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "- MAE: 0.6208  \n- RMSE: 1.1499  \n- R²: 0.5611"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## Training **MLP**"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "**MLP** training completed."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Saved **MLP** model to `mlp.joblib`."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "- MAE: 0.5612  \n- RMSE: 1.1041  \n- R²: 0.5953"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## Training **TransformerRegressor**"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Epoch 1 — Train Loss: 1.3754, Val Loss: 1.3382"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Saved best TransformerRegressor state to `best_transformer.pth`."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Epoch 2 — Train Loss: 1.3052, Val Loss: 1.2857"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Saved best TransformerRegressor state to `best_transformer.pth`."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Epoch 3 — Train Loss: 1.2861, Val Loss: 1.2897"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Epoch 4 — Train Loss: 1.2803, Val Loss: 1.2678"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Saved best TransformerRegressor state to `best_transformer.pth`."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Epoch 5 — Train Loss: 1.2716, Val Loss: 1.2633"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Saved best TransformerRegressor state to `best_transformer.pth`."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Epoch 6 — Train Loss: 1.2680, Val Loss: 1.2380"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Saved best TransformerRegressor state to `best_transformer.pth`."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Epoch 7 — Train Loss: 1.2621, Val Loss: 1.2292"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Saved best TransformerRegressor state to `best_transformer.pth`."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Epoch 8 — Train Loss: 1.2525, Val Loss: 1.2243"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Saved best TransformerRegressor state to `best_transformer.pth`."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Epoch 9 — Train Loss: 1.2452, Val Loss: 1.2053"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Saved best TransformerRegressor state to `best_transformer.pth`."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "Epoch 10 — Train Loss: 1.2393, Val Loss: 1.2128"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "### Transformer Training Logs"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "   Epoch  Train Loss  Val Loss\n0      1    1.375400  1.338154\n1      2    1.305162  1.285714\n2      3    1.286072  1.289695\n3      4    1.280303  1.267782\n4      5    1.271640  1.263299\n5      6    1.268009  1.238031\n6      7    1.262146  1.229211\n7      8    1.252480  1.224269\n8      9    1.245184  1.205259\n9     10    1.239338  1.212758",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Epoch</th>\n      <th>Train Loss</th>\n      <th>Val Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1.375400</td>\n      <td>1.338154</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1.305162</td>\n      <td>1.285714</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1.286072</td>\n      <td>1.289695</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1.280303</td>\n      <td>1.267782</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1.271640</td>\n      <td>1.263299</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>1.268009</td>\n      <td>1.238031</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>1.262146</td>\n      <td>1.229211</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>1.252480</td>\n      <td>1.224269</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>1.245184</td>\n      <td>1.205259</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>1.239338</td>\n      <td>1.212758</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## TransformerRegressor Evaluation"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "- MAE: 0.5956  \\- RMSE: 1.1306  \\- R²: 0.5757"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "## Summary of All Models"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "                  MAE      RMSE        R2\nModel                                    \nXGBoost      0.620842  1.149897  0.561067\nMLP          0.561194  1.104085  0.595344\nTransformer  0.595583  1.130591  0.575682",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MAE</th>\n      <th>RMSE</th>\n      <th>R2</th>\n    </tr>\n    <tr>\n      <th>Model</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>XGBoost</th>\n      <td>0.620842</td>\n      <td>1.149897</td>\n      <td>0.561067</td>\n    </tr>\n    <tr>\n      <th>MLP</th>\n      <td>0.561194</td>\n      <td>1.104085</td>\n      <td>0.595344</td>\n    </tr>\n    <tr>\n      <th>Transformer</th>\n      <td>0.595583</td>\n      <td>1.130591</td>\n      <td>0.575682</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "results = []\n",
    "\n",
    "simple_models = {\n",
    "    'XGBoost': XGBRegressor(tree_method='hist', random_state=42, verbosity=1),\n",
    "    'MLP': MLPRegressor(\n",
    "        hidden_layer_sizes=(64,32),\n",
    "        max_iter=500,\n",
    "        tol=1e-4,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "for name, model in simple_models.items():\n",
    "    display(Markdown(f'## Training **{name}**'))\n",
    "    model.fit(X_train_proc, y_train)\n",
    "    display(Markdown(f'**{name}** training completed.'))\n",
    "    \n",
    "    filename = f'{name.lower()}.joblib'\n",
    "    joblib.dump(model, filename)\n",
    "    display(Markdown(f'Saved **{name}** model to `{filename}`.'))\n",
    "\n",
    "    y_pred = model.predict(X_test_proc)\n",
    "    mae  = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2   = r2_score(y_test, y_pred)\n",
    "    display(Markdown(f'- MAE: {mae:.4f}  \\n- RMSE: {rmse:.4f}  \\n- R²: {r2:.4f}'))\n",
    "    results.append({'Model': name, 'MAE': mae, 'RMSE': rmse, 'R2': r2})\n",
    "\n",
    "class TransformerRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=64, nhead=8, num_layers=2, dim_feedforward=128):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.transformer(x)\n",
    "        x = x.squeeze(1)\n",
    "        return self.fc_out(x)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_dim = X_train_proc.shape[1]\n",
    "model_t = TransformerRegressor(input_dim, d_model=128, nhead=8,\n",
    "                               num_layers=2, dim_feedforward=256).to(device)\n",
    "optimizer = optim.Adam(model_t.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "epoch_logs = []\n",
    "\n",
    "display(Markdown('## Training **TransformerRegressor**'))\n",
    "for epoch in range(1, 11):\n",
    "    model_t.train()\n",
    "    epoch_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model_t(xb), yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "    model_t.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            val_losses.append(criterion(model_t(xb), yb).item())\n",
    "    avg_val_loss = np.mean(val_losses)\n",
    "    \n",
    "    epoch_logs.append({\n",
    "        'Epoch': epoch,\n",
    "        'Train Loss': avg_train_loss,\n",
    "        'Val Loss': avg_val_loss\n",
    "    })\n",
    "    display(Markdown(f'Epoch {epoch} — Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}'))\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model_t.state_dict(), 'best_transformer.pth')\n",
    "        display(Markdown('Saved best TransformerRegressor state to `best_transformer.pth`.'))\n",
    "\n",
    "logs_df = pd.DataFrame(epoch_logs)\n",
    "display(Markdown('### Transformer Training Logs'))\n",
    "display(logs_df)\n",
    "\n",
    "model_t.load_state_dict(torch.load('best_transformer.pth', map_location=device))\n",
    "model_t.eval()\n",
    "preds, true = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        preds.append(model_t(xb).cpu().numpy())\n",
    "        true.append(yb.numpy())\n",
    "preds = np.vstack(preds).squeeze()\n",
    "true  = np.vstack(true).squeeze()\n",
    "\n",
    "mae_t  = mean_absolute_error(true, preds)\n",
    "rmse_t = np.sqrt(mean_squared_error(true, preds))\n",
    "r2_t   = r2_score(true, preds)\n",
    "display(Markdown('## TransformerRegressor Evaluation'))\n",
    "display(Markdown(f'- MAE: {mae_t:.4f}  \\\\- RMSE: {rmse_t:.4f}  \\\\- R²: {r2_t:.4f}'))\n",
    "results.append({'Model': 'Transformer', 'MAE': mae_t, 'RMSE': rmse_t, 'R2': r2_t})\n",
    "\n",
    "results_df = pd.DataFrame(results).set_index('Model')\n",
    "display(Markdown('## Summary of All Models'))\n",
    "display(results_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-30T20:10:18.172837Z",
     "start_time": "2025-06-30T19:29:31.874343Z"
    }
   },
   "id": "c7173e338c9c8dfe",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['preprocessor.joblib']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(preprocessor, 'preprocessor.joblib')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-30T20:10:18.177686Z",
     "start_time": "2025-06-30T20:10:18.172837Z"
    }
   },
   "id": "45c03b70ddf4d4da",
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
