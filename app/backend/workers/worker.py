import json
import logging
import math
import os
import threading
import time

from datetime import datetime, timedelta, UTC

import numpy as np
from sklearn.preprocessing import StandardScaler
from workers.connection import get_rabbitmq_connection
from db.db import get_session
from services.user_manager import UserManager
from services.data_manager import DataManager
from services.core.enums import TaskStatus
from services.core.ml_model import ModelRegistry

LOGDIR = os.getenv('LOG_DIR', '/logs')
os.makedirs(LOGDIR, exist_ok=True)
logging.basicConfig(
    filename=os.path.join(LOGDIR, 'worker.log'),
    level=logging.INFO,
    format='%(asctime)s %(levelname)s %(message)s'
)

data = DataManager('data/lstm_data.csv')

def post_process(
    y_pred_s: np.ndarray,
    scaler_y: StandardScaler,
):
    y = y_pred_s.copy()
    y = scaler_y.inverse_transform(y.reshape(-1, 1)).reshape(y.shape)
    y = np.clip(y, 0, None)
    return [math.ceil(el) for el in y]

def process_prediction(task_data):
    with next(get_session()) as session:
        um = UserManager(session, task_data['user_id'])
        pred = um.prediction.get_by_id(task_data['prediction_id'])
        try:
            pred.status = TaskStatus.PROCESSING
            pred.timestamp = datetime.now(UTC)
            session.commit()

            model = ModelRegistry.get(task_data['model'])
            # –¥–æ–±–∞–≤–∏—Ç—å task_data['year'], task_data['month'], task_data['day'], task_data['hour']
            target_datetime = datetime(2024, 5, 30, 1)
            sequence = data.create_single_sequence(target_datetime, task_data['district'], model.scaler_X)

            result = model.predict(sequence)

            pred.result = str(post_process(result, model.scaler_y))
            pred.timestamp = datetime.now(UTC)
            pred.status = TaskStatus.COMPLETED
            session.commit()
            if task_data['cost'] != 0:
                um.balance.withdraw(task_data['cost'], description='–û–ø–ª–∞—Ç–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è')

            logging.info(f'‚úÖ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ: ID {task_data["prediction_id"]}, —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {pred.result}')

        except Exception as e:
            logging.exception(f'–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}')
            pred.status = TaskStatus.FAILED
            pred.timestamp = datetime.now(UTC)
            session.commit()


def callback(ch, method, properties, body):
    task_data = json.loads(body)
    logging.info(f'üì© –ü–æ–ª—É—á–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ: {task_data}')
    process_prediction(task_data)
    ch.basic_ack(delivery_tag=method.delivery_tag)


def start_worker():
    try:
        connection, channel, queue = get_rabbitmq_connection()
        channel.basic_qos(prefetch_count=1)
        channel.basic_consume(queue=queue, on_message_callback=callback)

        logging.info('[Worker –∑–∞–ø—É—â–µ–Ω –∏ –æ–∂–∏–¥–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–π...')
        channel.start_consuming()

    except Exception as e:
        logging.exception(f'Worker –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π: {e}')


def daily_reload():
    while True:
        now = datetime.now(UTC)
        tomorrow = (now + timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)
        delay = (tomorrow - now).total_seconds()
        time.sleep(delay)
        try:
            ModelRegistry.reload_all()
            logging.info('üîÑ –ú–æ–¥–µ–ª–∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ –≤–æ—Ä–∫–µ—Ä–µ')
        except Exception as e:
            logging.exception(f'–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–µ–π: {e}')


if __name__ == '__main__':
    try:
        ModelRegistry.reload_all()
        logging.info('‚úÖ –ú–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ –≤–æ—Ä–∫–µ—Ä–∞')
    except Exception as e:
        logging.exception(f'–û—à–∏–±–∫–∞ –ø—Ä–∏ –Ω–∞—á–∞–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–µ–π: {e}')

    thread = threading.Thread(target=daily_reload, daemon=True)
    thread.start()

    start_worker()
